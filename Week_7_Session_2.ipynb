{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 Session 2: Tracing Function Calling & Streamlit Apps\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this session, you will:\n",
    "1. Trace function calling with Langfuse for full visibility\n",
    "2. Build a traced Streamlit chatbot with tools\n",
    "3. Debug AI behavior by examining traces\n",
    "4. Understand how to monitor production applications\n",
    "5. Use traces to improve AI reliability\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Week 7 Session 1 (Langfuse basics)\n",
    "- Week 5 knowledge (function calling)\n",
    "- Week 6 knowledge (Streamlit)\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "**Session 1**: You learned to trace simple functions\n",
    "\n",
    "**Session 2**: Now trace REAL applications!\n",
    "\n",
    "### Real-World Debugging Scenarios\n",
    "\n",
    "**Problem 1**: \"My AI chatbot called the wrong tool!\"\n",
    "- âŒ Without tracing: Guess why it happened\n",
    "- âœ… With tracing: See exactly what the AI received and why it chose that tool\n",
    "\n",
    "**Problem 2**: \"Function calling works in notebook but fails in Streamlit!\"\n",
    "- âŒ Without tracing: Add print statements everywhere\n",
    "- âœ… With tracing: Compare traces from both environments\n",
    "\n",
    "**Problem 3**: \"AI sometimes gives wrong calculations!\"\n",
    "- âŒ Without tracing: Cannot reproduce the issue\n",
    "- âœ… With tracing: See the exact tool calls and parameters\n",
    "\n",
    "**This session teaches you to build observable AI apps!** ðŸš€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Make sure you completed Session 1 setup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from langfuse import observe, get_client\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Gemini\n",
    "client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "MODEL = \"gemini-2.5-flash-lite\"\n",
    "\n",
    "print(\"âœ… All imports ready!\")\n",
    "print(\"âœ… Langfuse configured!\")\n",
    "print(\"ðŸ” All function calls will be traced automatically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Tracing Function Calling\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "When AI uses tools, you need to see:\n",
    "1. **Which tool** the AI chose to call\n",
    "2. **What parameters** it extracted from user input\n",
    "3. **What result** the tool returned\n",
    "4. **How the AI** used that result in its response\n",
    "\n",
    "Without tracing, this is a **complete black box**! ðŸ“¦âŒ\n",
    "\n",
    "### Creating Traced Tools\n",
    "\n",
    "Let's create some simple tools and trace them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe()  # Trace this tool!\n",
    "def calculate_tip(bill_amount: float, tip_percentage: float) -> float:\n",
    "    \"\"\"Calculate tip amount based on bill and percentage.\n",
    "    \n",
    "    Args:\n",
    "        bill_amount: Total bill before tip in dollars\n",
    "        tip_percentage: Tip percentage (e.g., 15 for 15%)\n",
    "    \n",
    "    Returns:\n",
    "        Tip amount in dollars\n",
    "    \"\"\"\n",
    "    result = bill_amount * (tip_percentage / 100)\n",
    "    print(f\"ðŸ’° Calculating tip: ${bill_amount} Ã— {tip_percentage}% = ${result:.2f}\")\n",
    "    return result\n",
    "\n",
    "@observe()  # Trace this too!\n",
    "def split_bill(total_amount: float, num_people: int) -> float:\n",
    "    \"\"\"Split a bill evenly among people.\n",
    "    \n",
    "    Args:\n",
    "        total_amount: Total amount to split in dollars\n",
    "        num_people: Number of people to split among\n",
    "    \n",
    "    Returns:\n",
    "        Amount per person in dollars\n",
    "    \"\"\"\n",
    "    if num_people <= 0:\n",
    "        return 0.0\n",
    "    result = total_amount / num_people\n",
    "    print(f\"ðŸ§® Splitting ${total_amount} among {num_people} people = ${result:.2f} each\")\n",
    "    return result\n",
    "\n",
    "@observe()  # And this!\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers.\n",
    "    \n",
    "    Args:\n",
    "        a: First number\n",
    "        b: Second number\n",
    "    \n",
    "    Returns:\n",
    "        Product of a and b\n",
    "    \"\"\"\n",
    "    result = a * b\n",
    "    print(f\"âœ–ï¸  Multiplying {a} Ã— {b} = {result}\")\n",
    "    return result\n",
    "\n",
    "print(\"âœ… Tools defined with tracing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Tools with AI (Traced)\n",
    "\n",
    "Now let's create a traced function that uses these tools with Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe()  # Trace the AI interaction\n",
    "def ask_with_tools(prompt: str) -> str:\n",
    "    \"\"\"Ask Gemini with access to tools.\"\"\"\n",
    "    \n",
    "    # Create chat with tools\n",
    "    chat = client.chats.create(\n",
    "        model=MODEL,\n",
    "        config={\n",
    "            'tools': [calculate_tip, split_bill, multiply],\n",
    "            'system_instruction': 'You are a helpful assistant with access to calculation tools. Use them when appropriate.'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Send message\n",
    "    response = chat.send_message(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Test it\n",
    "print(\"ðŸ”„ Testing traced function calling...\\n\")\n",
    "result = ask_with_tools(\"The bill is $85 and I want to leave a 20% tip. How much is the tip?\")\n",
    "print(f\"\\nðŸ“¤ AI Response: {result}\")\n",
    "print(\"\\nâœ… Check Langfuse dashboard to see the FULL trace!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Explore the Trace\n",
    "\n",
    "**Go to Langfuse dashboard now!**\n",
    "\n",
    "You'll see a tree like this:\n",
    "\n",
    "```\n",
    "ask_with_tools (parent)\n",
    "â””â”€â”€ calculate_tip\n",
    "    â””â”€â”€ Input: {bill_amount: 85.0, tip_percentage: 20.0}\n",
    "    â””â”€â”€ Output: 17.0\n",
    "```\n",
    "\n",
    "**What you can see:**\n",
    "1. âœ… The user's original prompt\n",
    "2. âœ… Which tool the AI decided to call (`calculate_tip`)\n",
    "3. âœ… The exact parameters it extracted (`bill_amount=85.0, tip_percentage=20.0`)\n",
    "4. âœ… The tool's return value (`17.0`)\n",
    "5. âœ… The AI's final response using that value\n",
    "\n",
    "**This is debugging gold!** âœ¨\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Complex Multi-Tool Scenarios\n",
    "\n",
    "Let's try something harder - a query that needs MULTIPLE tool calls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”„ Testing multi-tool scenario...\\n\")\n",
    "\n",
    "result = ask_with_tools(\n",
    "    \"The bill is $120, I want an 18% tip, and we're 3 people. How much does each person pay?\"\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“¤ AI Response: {result}\")\n",
    "print(\"\\nâœ… Check Langfuse - you'll see MULTIPLE tool calls!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ What You'll See in Langfuse\n",
    "\n",
    "```\n",
    "ask_with_tools\n",
    "â”œâ”€â”€ calculate_tip\n",
    "â”‚   â””â”€â”€ Input: {bill_amount: 120.0, tip_percentage: 18.0}\n",
    "â”‚   â””â”€â”€ Output: 21.6\n",
    "â””â”€â”€ split_bill\n",
    "    â””â”€â”€ Input: {total_amount: 141.6, num_people: 3}\n",
    "    â””â”€â”€ Output: 47.2\n",
    "```\n",
    "\n",
    "**Notice:**\n",
    "- The AI called TWO tools\n",
    "- It calculated tip first: $120 Ã— 18% = $21.60\n",
    "- Then split total: ($120 + $21.60) Ã· 3 = $47.20\n",
    "- All automatically traced!\n",
    "\n",
    "### Why This Is Powerful for Debugging\n",
    "\n",
    "**Scenario**: User reports \"The split bill calculation is wrong!\"\n",
    "\n",
    "**Without tracing**: \n",
    "- âŒ Guess which values the AI used\n",
    "- âŒ Can't see if AI made wrong assumptions\n",
    "- âŒ Hard to reproduce the issue\n",
    "\n",
    "**With tracing**:\n",
    "- âœ… See the EXACT parameters AI extracted\n",
    "- âœ… Verify each tool's output\n",
    "- âœ… Identify if problem is in tool logic or AI interpretation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Tracing Multi-Turn Conversations with Tools\n",
    "\n",
    "Now let's combine everything: multi-turn chat + function calling + full tracing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe()  # Trace each individual turn\n",
    "def send_message_with_tools(chat, message: str, turn_number: int) -> str:\n",
    "    \"\"\"Send a single message with tool access - traced!\"\"\"\n",
    "    print(f\"\\nðŸ‘¤ User [{turn_number}]: {message}\")\n",
    "    response = chat.send_message(message)\n",
    "    print(f\"ðŸ¤– AI [{turn_number}]: {response.text}\")\n",
    "    return response.text\n",
    "\n",
    "@observe()  # Trace the entire conversation\n",
    "def have_tool_conversation(messages: list[str], session_id: str = None) -> list[str]:\n",
    "    \"\"\"Multi-turn conversation with tool access.\"\"\"\n",
    "    \n",
    "    # Add session tracking\n",
    "    if session_id:\n",
    "        langfuse = get_client()\n",
    "        langfuse.update_current_trace(\n",
    "            session_id=session_id,\n",
    "            tags=[\"conversation\", \"tool-calling\", \"multi-turn\"]\n",
    "        )\n",
    "    \n",
    "    # Create chat with tools\n",
    "    chat = client.chats.create(\n",
    "        model=MODEL,\n",
    "        config={\n",
    "            'tools': [calculate_tip, split_bill, multiply],\n",
    "            'system_instruction': 'You are a helpful restaurant assistant with calculation tools. Always provide an answer to the user'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Each turn gets its own trace\n",
    "    responses = []\n",
    "    for i, message in enumerate(messages, 1):\n",
    "        response = send_message_with_tools(chat, message, i)\n",
    "        responses.append(response)\n",
    "    \n",
    "    return responses\n",
    "\n",
    "# Test it\n",
    "print(\"ðŸ”„ Starting multi-turn conversation with tools...\")\n",
    "\n",
    "conversation = [\n",
    "    \"Hi! Our bill is $150\",\n",
    "    \"Can you calculate a 20% tip?\",\n",
    "    \"Great! Now split the total among 4 people\"\n",
    "]\n",
    "\n",
    "responses = have_tool_conversation(conversation, session_id=\"restaurant_123\")\n",
    "print(\"\\nâœ… Conversation traced! Check Langfuse dashboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ The Full Trace Structure\n",
    "\n",
    "**In Langfuse, you'll see:**\n",
    "\n",
    "```\n",
    "have_tool_conversation (session: restaurant_123)\n",
    "â”œâ”€â”€ send_message_with_tools (turn 1)\n",
    "â”‚   â””â”€â”€ No tools called (just greeting)\n",
    "â”œâ”€â”€ send_message_with_tools (turn 2)\n",
    "â”‚   â””â”€â”€ calculate_tip\n",
    "â”‚       â””â”€â”€ Input: {bill_amount: 150.0, tip_percentage: 20.0}\n",
    "â”‚       â””â”€â”€ Output: 30.0\n",
    "â””â”€â”€ send_message_with_tools (turn 3)\n",
    "    â””â”€â”€ split_bill\n",
    "        â””â”€â”€ Input: {total_amount: 180.0, num_people: 4}\n",
    "        â””â”€â”€ Output: 45.0\n",
    "```\n",
    "\n",
    "### Why This Structure Matters\n",
    "\n",
    "**For debugging:**\n",
    "- ðŸ” See which turn used which tools\n",
    "- ðŸ” Track conversation context across turns\n",
    "- ðŸ” Verify AI remembered information from earlier turns\n",
    "\n",
    "**For monitoring:**\n",
    "- ðŸ“Š Track how many tools are called per conversation\n",
    "- ðŸ“Š Identify conversations with errors\n",
    "- ðŸ“Š Measure performance per turn\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Building a Traced Streamlit App\n",
    "\n",
    "Now let's take everything we learned and build a **production-ready** traced chatbot!\n",
    "\n",
    "### The App Structure\n",
    "\n",
    "We'll create a Streamlit app that:\n",
    "1. âœ… Has a chat interface\n",
    "2. âœ… Uses multiple tools\n",
    "3. âœ… Traces EVERY interaction\n",
    "4. âœ… Shows trace info in the UI\n",
    "\n",
    "### Why Streamlit + Tracing?\n",
    "\n",
    "**Problem**: Streamlit apps are hard to debug\n",
    "- State management is complex\n",
    "- Errors happen in production\n",
    "- Users can't explain what went wrong\n",
    "\n",
    "**Solution**: Trace everything!\n",
    "- See exactly what user typed\n",
    "- See exactly what AI did\n",
    "- Reproduce issues easily\n",
    "\n",
    "### The Code\n",
    "\n",
    "I've created a complete Streamlit app for you in:\n",
    "```\n",
    "Week_7/session_2_apps/traced_chat_app.py\n",
    "```\n",
    "\n",
    "**To run it:**\n",
    "```bash\n",
    "uv run streamlit run Week_7/session_2_apps/traced_chat_app.py\n",
    "```\n",
    "\n",
    "### What Makes It Special?\n",
    "\n",
    "Every single interaction is traced:\n",
    "- User messages\n",
    "- AI responses\n",
    "- Tool calls\n",
    "- Errors\n",
    "\n",
    "**Try these in the app:**\n",
    "1. \"Calculate a 20% tip on $85\"\n",
    "2. \"What's 47 times 23?\"\n",
    "3. \"The bill is $120, I want 18% tip split among 3 people\"\n",
    "\n",
    "Then check Langfuse to see **everything** that happened!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Debugging Real Issues with Traces\n",
    "\n",
    "Let's see how tracing helps debug actual problems.\n",
    "\n",
    "### Scenario 1: Wrong Tool Called\n",
    "\n",
    "**User complaint**: \"I asked for a tip calculation but got multiplication!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliberately ambiguous prompt\n",
    "print(\"ðŸ”„ Testing ambiguous prompt...\\n\")\n",
    "\n",
    "result = ask_with_tools(\"What's 15 on 100?\")\n",
    "print(f\"\\nðŸ“¤ AI Response: {result}\")\n",
    "print(\"\\nðŸ” Check Langfuse: Which tool did it call?\")\n",
    "print(\"ðŸ’¡ The trace shows you EXACTLY what the AI understood!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug Process\n",
    "\n",
    "1. **Check the trace** in Langfuse\n",
    "2. **See which tool** was called\n",
    "3. **See what parameters** were extracted\n",
    "4. **Understand** why AI made that choice\n",
    "5. **Fix** by improving prompt or tool descriptions\n",
    "\n",
    "### Scenario 2: Tool Returns Error\n",
    "\n",
    "What happens when a tool fails?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should cause an issue\n",
    "print(\"ðŸ”„ Testing edge case...\\n\")\n",
    "\n",
    "result = ask_with_tools(\"Split $100 among 0 people\")\n",
    "print(f\"\\nðŸ“¤ AI Response: {result}\")\n",
    "print(\"\\nðŸ” Check Langfuse: Did the tool return 0? How did AI handle it?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What the Trace Shows\n",
    "\n",
    "In Langfuse:\n",
    "- âœ… See the tool was called with `num_people=0`\n",
    "- âœ… See it returned `0.0`\n",
    "- âœ… See how AI explained this to the user\n",
    "\n",
    "**Without tracing**: You'd never know the tool was actually called!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Production Monitoring Patterns\n",
    "\n",
    "### Pattern 1: User Identification\n",
    "\n",
    "Always tag traces with user info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe()\n",
    "def production_ask(prompt: str, user_id: str, environment: str = \"production\") -> str:\n",
    "    \"\"\"Production-ready function with full metadata.\"\"\"\n",
    "    \n",
    "    # Add comprehensive metadata\n",
    "    langfuse = get_client()\n",
    "    langfuse.update_current_trace(\n",
    "        user_id=user_id,\n",
    "        tags=[environment, \"function-calling\"],\n",
    "        metadata={\n",
    "            \"model\": MODEL,\n",
    "            \"tools_available\": [\"calculate_tip\", \"split_bill\", \"multiply\"],\n",
    "            \"version\": \"2.0\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Create chat with tools\n",
    "    chat = client.chats.create(\n",
    "        model=MODEL,\n",
    "        config={\n",
    "            'tools': [calculate_tip, split_bill, multiply],\n",
    "            'system_instruction': 'You are a helpful assistant with calculation tools.'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    response = chat.send_message(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Test with metadata\n",
    "result = production_ask(\n",
    "    \"Calculate 15% tip on $200\",\n",
    "    user_id=\"customer_456\",\n",
    "    environment=\"testing\"\n",
    ")\n",
    "\n",
    "print(f\"Result: {result}\")\n",
    "print(\"\\nâœ… Check Langfuse - filter by user_id to see ALL their traces!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Error Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe()\n",
    "def safe_ask(prompt: str) -> dict:\n",
    "    \"\"\"Ask with comprehensive error handling.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        chat = client.chats.create(\n",
    "            model=MODEL,\n",
    "            config={\n",
    "                'tools': [calculate_tip, split_bill, multiply],\n",
    "                'system_instruction': 'You are a helpful assistant.'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        response = chat.send_message(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"response\": response.text,\n",
    "            \"error\": None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Error is automatically logged in trace!\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"response\": None,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Test\n",
    "result = safe_ask(\"Calculate tip on $50 at 15%\")\n",
    "print(f\"Success: {result['success']}\")\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(\"\\nâœ… Even errors are traced!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: A/B Testing\n",
    "\n",
    "Use tags to compare different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe()\n",
    "def ask_variant_a(prompt: str) -> str:\n",
    "    \"\"\"Variant A: Shorter system instruction.\"\"\"\n",
    "    langfuse = get_client()\n",
    "    langfuse.update_current_trace(tags=[\"experiment\", \"variant-a\", \"short-instruction\"])\n",
    "    \n",
    "    chat = client.chats.create(\n",
    "        model=MODEL,\n",
    "        config={\n",
    "            'tools': [calculate_tip, split_bill, multiply],\n",
    "            'system_instruction': 'You are helpful.'\n",
    "        }\n",
    "    )\n",
    "    return chat.send_message(prompt).text\n",
    "\n",
    "@observe()\n",
    "def ask_variant_b(prompt: str) -> str:\n",
    "    \"\"\"Variant B: Detailed system instruction.\"\"\"\n",
    "    langfuse = get_client()\n",
    "    langfuse.update_current_trace(tags=[\"experiment\", \"variant-b\", \"detailed-instruction\"])\n",
    "    \n",
    "    chat = client.chats.create(\n",
    "        model=MODEL,\n",
    "        config={\n",
    "            'tools': [calculate_tip, split_bill, multiply],\n",
    "            'system_instruction': 'You are a helpful restaurant assistant. Use tools when appropriate to provide accurate calculations.'\n",
    "        }\n",
    "    )\n",
    "    return chat.send_message(prompt).text\n",
    "\n",
    "# Test both\n",
    "test_prompt = \"I have a $100 bill and want 20% tip for 2 people\"\n",
    "\n",
    "print(\"Testing variant A...\")\n",
    "result_a = ask_variant_a(test_prompt)\n",
    "print(f\"A: {result_a}\\n\")\n",
    "\n",
    "print(\"Testing variant B...\")\n",
    "result_b = ask_variant_b(test_prompt)\n",
    "print(f\"B: {result_b}\\n\")\n",
    "\n",
    "print(\"âœ… Compare in Langfuse: Filter by 'variant-a' vs 'variant-b' tags!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Practice Exercises\n",
    "\n",
    "### Exercise 1: Add More Tools\n",
    "\n",
    "**Goal**: Add 2 new tools and trace them.\n",
    "\n",
    "**TODO**:\n",
    "1. Create a `subtract(a, b)` tool\n",
    "2. Create a `calculate_discount(price, discount_percent)` tool\n",
    "3. Add `@observe()` to both\n",
    "4. Test them with the AI\n",
    "5. Check traces in Langfuse\n",
    "\n",
    "**Success criteria**: See both tools traced in Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Build a Shopping Assistant\n",
    "\n",
    "**Goal**: Create a traced shopping assistant with tools.\n",
    "\n",
    "**TODO**:\n",
    "1. Create tools: `calculate_discount`, `calculate_tax`, `calculate_shipping`\n",
    "2. Build a `shopping_assistant` function with these tools\n",
    "3. Add proper metadata (user_id, session_id)\n",
    "4. Test with: \"I'm buying a $250 item with 10% discount, 8% tax, and $15 shipping\"\n",
    "\n",
    "**Success criteria**: See all 3 tools called in one trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Debug a Problem\n",
    "\n",
    "**Goal**: Use traces to find and fix an issue.\n",
    "\n",
    "**TODO**:\n",
    "1. Create a tool that sometimes fails (e.g., validates input)\n",
    "2. Run it with both valid and invalid inputs\n",
    "3. Use Langfuse to see what happened\n",
    "4. Improve error handling based on what you see\n",
    "\n",
    "**Success criteria**: Can explain the failure using traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### âœ… What You Learned\n",
    "\n",
    "1. **Function calling is traceable**\n",
    "   - Every tool call is logged\n",
    "   - See parameters and return values\n",
    "   - Understand AI's decision-making\n",
    "\n",
    "2. **Multi-turn conversations need structure**\n",
    "   - Each turn = separate trace\n",
    "   - Parent trace groups the session\n",
    "   - Track context across turns\n",
    "\n",
    "3. **Streamlit apps benefit from tracing**\n",
    "   - Debug production issues\n",
    "   - See what users actually did\n",
    "   - Reproduce problems easily\n",
    "\n",
    "4. **Metadata is critical**\n",
    "   - Tag with user_id\n",
    "   - Tag with environment\n",
    "   - Add version info\n",
    "   - Enable filtering and analysis\n",
    "\n",
    "5. **Errors are traced too**\n",
    "   - Failed tool calls are visible\n",
    "   - Exceptions are logged\n",
    "   - Debug faster\n",
    "\n",
    "### ðŸŽ¯ Production Checklist\n",
    "\n",
    "When deploying traced apps:\n",
    "\n",
    "- âœ… Add `@observe()` to all AI functions\n",
    "- âœ… Add `@observe()` to all tool functions\n",
    "- âœ… Include user_id in traces\n",
    "- âœ… Tag with environment (dev/staging/prod)\n",
    "- âœ… Add version info in metadata\n",
    "- âœ… Set up error handling\n",
    "- âœ… Test trace visibility in dashboard\n",
    "- âœ… Set up alerts for errors (Langfuse feature)\n",
    "\n",
    "### ðŸ“š Next Steps\n",
    "\n",
    "Now you can:\n",
    "- Build traced applications confidently\n",
    "- Debug AI behavior systematically\n",
    "- Monitor production apps\n",
    "- Compare different approaches\n",
    "- Improve AI reliability\n",
    "\n",
    "### ðŸŽ¯ Resources\n",
    "\n",
    "- **Langfuse Dashboard**: [https://cloud.langfuse.com](https://cloud.langfuse.com)\n",
    "- **Documentation**: [https://langfuse.com/docs](https://langfuse.com/docs)\n",
    "- **Function Calling**: [https://langfuse.com/docs/sdk/python/decorators](https://langfuse.com/docs/sdk/python/decorators)\n",
    "\n",
    "**You can now build production-ready AI applications! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
